[
  {
    "objectID": "index.html#what-is-this",
    "href": "index.html#what-is-this",
    "title": "Deep Learning Notes",
    "section": "What is this?",
    "text": "What is this?\nSome notes made from the NPTEL course on deep learning (CS7015 2018).\n\n\n\n\nCS7015. 2018. https://nptel.ac.in/courses/106106184."
  },
  {
    "objectID": "artificial-neuron/mcculloch-pitts-neuron.html#a-basic-mathematical-formulation",
    "href": "artificial-neuron/mcculloch-pitts-neuron.html#a-basic-mathematical-formulation",
    "title": "1  The McCulloch-Pitts Neuron",
    "section": "1.1 A basic mathematical formulation",
    "text": "1.1 A basic mathematical formulation\nLet \\(f_{\\theta, N}\\) be a boolean function (Section 6.1) where \\(\\theta\\) is a non-negative real constant called the threshold and \\(N\\) is the set of indices (of the input binary tuple) corresponding to inhibitory inputs. We define \\(f_{\\theta, N}\\) in the following manner, \\[\nf_{\\theta, N}(\\textbf{x}) =\n\\begin{cases}\n1 \\text{ if } \\sum\\limits_{i = 1}^n x_i \\geq \\theta \\text { and } x_i = 0 \\;\\forall\\; i \\in N \\\\\n0 \\text{ if } \\sum\\limits_{i = 1}^n x_i < \\theta \\text{ or } \\;\\exists\\; i \\in N \\text{ s.t } x_i = 1\n\\end{cases}\n\\]\nhere \\(\\textbf{x}\\) is an input boolean tuple and \\(x_i\\) is the \\(i^{th}\\) element of \\(\\textbf{x}\\).\nBelow is an implementation of the McCulloch pitts neuron.\n\nimport torch\n\ndef McCullochPittsNeuron(x, N, theta):\n    val = torch.sum(x)\n    if (val >= theta):\n        if (torch.sum(torch.index_select(x, 0, N)) == 0):\n            return 1\n    return 0"
  },
  {
    "objectID": "artificial-neuron/mcculloch-pitts-neuron.html#fitting-some-boolean-functions",
    "href": "artificial-neuron/mcculloch-pitts-neuron.html#fitting-some-boolean-functions",
    "title": "1  The McCulloch-Pitts Neuron",
    "section": "1.2 Fitting some boolean functions",
    "text": "1.2 Fitting some boolean functions\nThe MP Neuron can be thought of as a boolean function approximator. An MP Neuron can match the truth table of a boolean function (at least partially) by adjusting the values of \\(\\theta\\) and \\(N\\). The process of finding the values of \\(\\theta\\) and \\(N\\) that result in the best match is called fitting.\n\nimport itertools\nfrom matplotlib import pyplot as plt\n\ndef MPFunction(inp, N, theta):\n    y = []\n    for x in inp:\n        y.append(McCullochPittsNeuron(x, torch.tensor(N, dtype=torch.int32), theta))\n    return y\n\ndef plot_MPFunction(theta, N = ()):\n    inputs = torch.tensor(list(itertools.product((0, 1), repeat = 2)), dtype = torch.int32)\n    outputs = torch.tensor(MPFunction(inputs, N, theta), dtype = torch.int32)\n    off_values = inputs[outputs == 0]\n    on_values = inputs[outputs == 1]\n\n    plt.scatter(off_values[:, 0], off_values[:, 1])\n    plt.scatter(on_values[:, 0], on_values[:, 1])\n\nFor OR,\n\n    plot_MPFunction(1) # theta = 1 for OR\n    plt.show()\n\n\n\n\nFor AND,\n\n    plot_MPFunction(2) # theta = 2 for AND\n    plt.show()\n\n\n\n\nFor NAND,\n\n    plot_MPFunction(0, (0, 1))\n    plt.show()\n\n\n\n\nThere are many boolean functions which the MP Neuron cannot fit.\n\n1.2.1 A systematic method to fit\nYou must also prove that this method is definately the most effecient way to do things."
  },
  {
    "objectID": "artificial-neuron/mcculloch-pitts-neuron.html#geometric-interpretation",
    "href": "artificial-neuron/mcculloch-pitts-neuron.html#geometric-interpretation",
    "title": "1  The McCulloch-Pitts Neuron",
    "section": "1.3 Geometric interpretation",
    "text": "1.3 Geometric interpretation\nThe geometrical interpretation of the MP Neuron can be found by looking at it’s decision boundary. The decision boundary in the case where \\(N=\\phi\\) is given by the equation, \\[\n    \\sum\\limits_{i = 1}^n x_i = \\theta\n\\]\nthis is nothing but a straight line in the cartesian plane when \\(n = 2\\). Below is a plot of thE decision boundary when trying to fit the AND and OR function.\n\nplot_MPFunction(2)\nline_xs = torch.arange(0, 200)/100\nline_ys = 2 - line_xs\nplt.plot(line_xs, line_ys)\nplt.show()\n\nplot_MPFunction(1)\nline_xs = torch.arange(0, 100)/100\nline_ys = 1 - line_xs\nplt.plot(line_xs, line_ys)\nplt.show()\n\n\n\n\n\n\n\nBy adjusting the value of \\(\\theta\\) we can only change the perpendicular distance between the linear decision boundary and the origin. However, if \\(N\\neq\\phi\\) then the decision boundary becomes non-linear."
  },
  {
    "objectID": "artificial-neuron/mcculloch-pitts-neuron.html#limitations",
    "href": "artificial-neuron/mcculloch-pitts-neuron.html#limitations",
    "title": "1  The McCulloch-Pitts Neuron",
    "section": "1.4 Limitations",
    "text": "1.4 Limitations\n\nIn many practical applications, relevant information will often be non-boolean.\nNot all inputs have the same importance in making a decision\nCannot fit functions which are not linearly seperable.\n\n\n\n\n\nMcCulloch, and Pitts. 1943. https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf."
  },
  {
    "objectID": "artificial-neuron/perceptron.html#precise-mathematical-formulation",
    "href": "artificial-neuron/perceptron.html#precise-mathematical-formulation",
    "title": "2  The Perceptron",
    "section": "2.1 Precise mathematical formulation",
    "text": "2.1 Precise mathematical formulation\nThe perceptron is a function \\(f_{\\theta, \\textbf{w}}:\\mathbb{R}^n\\to\\{0, 1\\}\\) where \\(\\theta > 0\\) is a real constant and \\(\\textbf{w} \\in \\mathbb{R}^n\\) is a real-valued vector, called the weights of the model. \\[\nf_{\\theta, \\textbf{w}}(\\textbf{x}) =\n\\begin{cases}\n    1 \\text{ if } \\textbf{w}\\cdot\\textbf{x} \\geq \\theta \\\\\n    0 \\text{ if } \\textbf{w}\\cdot\\textbf{x} < \\theta\n\\end{cases}\n\\]\nFor the sake of analysis we ofter use the equivalent definition,\n\\[\nf_{\\theta, \\textbf{w}'}(\\textbf{x}') =\n\\begin{cases}\n    1 \\text { if } \\textbf{w}'\\cdot\\textbf{x}' \\geq 0 \\\\\n    0 \\text { if } \\textbf{w}'\\cdot\\textbf{x}' < 0\n\\end{cases}\n\\]\nhere \\(\\textbf{w}' \\in\\mathbb{R}^{n+1}\\) is nothing but a concatenation of \\(\\textbf{w}\\) and singleton \\(-\\theta\\). Similarly, \\(\\textbf{x}' \\in\\mathbb{R}^{n+1}\\) is a concatination of \\(\\textbf{x}\\) and singleton \\(1\\). Both \\(-\\theta\\) and \\(1\\) are in corresponding positions.\n\n2.1.1 Geometrical interpretation\nThe seperating boundary of the perceptron is a general hyperplane in \\(\\mathbb{R}^n\\) for a input vector of \\(n\\)-dimensions. Note that hyperplane does not have to be origin centered. The \\(\\theta\\) term in fact gives the perpendicular distance of the origin with the hyperplane. The MP Neuron was always parallel to the hyperplane given by the equation \\(\\textbf{x}'\\cdot I = 0\\)."
  },
  {
    "objectID": "artificial-neuron/perceptron.html#differences-between-the-perceptron-and-the-mp-neuron",
    "href": "artificial-neuron/perceptron.html#differences-between-the-perceptron-and-the-mp-neuron",
    "title": "2  The Perceptron",
    "section": "2.2 Differences between the perceptron and the MP-neuron",
    "text": "2.2 Differences between the perceptron and the MP-neuron\n\nThe perceptron can accept real-valued inputs whereas the MP neuron cannot.\nThe perceptron can assign an importance to each input. Geometrically, this can be interpreted by the fact the perceptron has an arbitrary hyperplane as its seperating boundary instead of being restricted to a family of parallel hyperplanes."
  },
  {
    "objectID": "artificial-neuron/perceptron.html#perceptron-learning-algorithm",
    "href": "artificial-neuron/perceptron.html#perceptron-learning-algorithm",
    "title": "2  The Perceptron",
    "section": "2.3 Perceptron learning algorithm",
    "text": "2.3 Perceptron learning algorithm\nThe perceptron learning algorithm gives a way in which the parameters of the perceptron can be adjusted iteratively, to successfully seperate any linearly seperable (Section 7.1) and bounded set of points (Section 7.2) in a finite albeit unknown number of steps.\nLet X be a set of linearly seperable and bounded points.\n\nIf there exists x in X such that \n    y(w'x') < 0\nthen\n    w' = w' + yx\nelse\n    terminate"
  },
  {
    "objectID": "artificial-neuron/perceptron.html#proof-of-convergence",
    "href": "artificial-neuron/perceptron.html#proof-of-convergence",
    "title": "2  The Perceptron",
    "section": "2.4 Proof of convergence",
    "text": "2.4 Proof of convergence\nA perceptron is said to have converged on a set of points \\(X\\) iff \\(y(w'x') > 0 \\;\\forall\\; x\\in X\\), in such a case we say that the perceptron has fit the underlying boolean function. We now show that the above algorithm indeed makes the perceptron converge in a finite number of steps.\nSHOW PROOF OF CONVERGENCE."
  },
  {
    "objectID": "artificial-neuron/perceptron.html#network-of-perceptrons",
    "href": "artificial-neuron/perceptron.html#network-of-perceptrons",
    "title": "2  The Perceptron",
    "section": "2.5 Network of perceptrons",
    "text": "2.5 Network of perceptrons\nEven through a single-perceptron can only converge on a set of linearly seperable points, a network of perceptrons can converge on other functions too."
  },
  {
    "objectID": "artificial-neuron/perceptron.html#limitations-and-other-notes",
    "href": "artificial-neuron/perceptron.html#limitations-and-other-notes",
    "title": "2  The Perceptron",
    "section": "2.6 Limitations and other notes",
    "text": "2.6 Limitations and other notes\nThe only limitation of a single perceptron comes from the fact that it is unable to fit a set of points which is not linearly seperable. However, this problem is taken care of by the fact that a network of perceptrons is able to fit such a function."
  },
  {
    "objectID": "artificial-neuron/sigmoid-neuron.html#motivation",
    "href": "artificial-neuron/sigmoid-neuron.html#motivation",
    "title": "3  The sigmoid neuron",
    "section": "3.1 Motivation",
    "text": "3.1 Motivation\nThe sigmoid neuron addresses some limitations of the perceptron: 1. The perceptron could only fit binary functions 2. The perceptron is not a fully differentiable model. 3. Consider two inputs to a single-input perceptron: \\(\\theta+d\\theta\\) and \\(\\theta-d\\theta\\). Due to the nature of the thresholding logic, the perceptron will output \\(1\\) on the first and \\(0\\) on the second if regardless of how small \\(d\\theta\\) is."
  },
  {
    "objectID": "artificial-neuron/sigmoid-neuron.html#mathematical-formulation",
    "href": "artificial-neuron/sigmoid-neuron.html#mathematical-formulation",
    "title": "3  The sigmoid neuron",
    "section": "3.2 Mathematical formulation",
    "text": "3.2 Mathematical formulation\nThe sigmoid neuron is a real-valued function \\(f_{k, \\theta, \\textbf{w}}:\\mathbb{R}^n\\to\\mathbb{R}\\) where \\(k\\) and \\(\\theta\\) are positive real numbers and \\(\\textbf{w}\\) is a real valued vector such that \\[\nf_{k, \\theta, \\textbf{w}}(\\textbf{x}) =  g_k(\\theta + \\textbf{w}\\cdot\\textbf{x})\ng_k(\\textbf{y}) = \\frac{1}{1+e^{-ky}}\n\\]\n\\(g_k(\\textbf{y})\\) is a logistic function with a maximum value of \\(1\\) and steepness of \\(k\\). However, \\(g_k\\) is not just restricted to the logistic function, in fact it can take on any function belonging to the sigmoid family of functions. Another virtue of the sigmoid neuron is the fact that it is differentiable for all input values.\n\n3.2.1 Interpretation as probability\nSince the sigmoid neuron has a range of \\((0, 1)\\) it can be interpreted as a probability/liklehood/certainty of a particular event. This interpretation has many applications."
  },
  {
    "objectID": "artificial-neuron/modern-artificial-neuron.html",
    "href": "artificial-neuron/modern-artificial-neuron.html",
    "title": "4  The modern artificial neuron",
    "section": "",
    "text": "Unlike the previous models the modern artificial neurons abstracts away from the previous concrete models into a general template whose parts can be swapped in-and-out on the basis of necessity. We discuss the general template as well as some specific manifestations of the archetype.\ninformative article check: https://machinelearningknowledge.ai/artificial-neuron/"
  },
  {
    "objectID": "machine-learning/machine-learning-setup.html#a-discussion-on-model-convergence",
    "href": "machine-learning/machine-learning-setup.html#a-discussion-on-model-convergence",
    "title": "5  The machine learning setup",
    "section": "5.1 A Discussion on model convergence",
    "text": "5.1 A Discussion on model convergence\nThe precise definition of convergence of a model varies depending on the context. However, in general we say that a model has converged if we have roughly reached a set of parameters at which the loss function has a minima. It is not necessary that the minimum loss is 0, it is a luxury."
  },
  {
    "objectID": "mathematics/boolean-logic.html#sec-boolean_function",
    "href": "mathematics/boolean-logic.html#sec-boolean_function",
    "title": "6  Boolean Logic",
    "section": "6.1 Boolean function",
    "text": "6.1 Boolean function\nLet \\(X\\) be the set of all possible boolean \\(n\\)-tuples. Then, a function \\(f:X\\to \\{0, 1\\}\\) is called a boolean function with \\(n\\) inputs."
  },
  {
    "objectID": "mathematics/prerequisite-topology.html#sec-linearly_seperable_points",
    "href": "mathematics/prerequisite-topology.html#sec-linearly_seperable_points",
    "title": "7  Prerequisite Topology",
    "section": "7.1 Linearly serperable set of points",
    "text": "7.1 Linearly serperable set of points\nthere is something here."
  },
  {
    "objectID": "mathematics/prerequisite-topology.html#sec-bounded_points",
    "href": "mathematics/prerequisite-topology.html#sec-bounded_points",
    "title": "7  Prerequisite Topology",
    "section": "7.2 Bounded points",
    "text": "7.2 Bounded points\nthere is somethign here too."
  },
  {
    "objectID": "pytorch-experiments/basic_tensor_one.html",
    "href": "pytorch-experiments/basic_tensor_one.html",
    "title": "9  Tensor operation exercises",
    "section": "",
    "text": "10 Why are there discrepensies with the precision digit after the decimal point?\nIt is because of float16 is less precise than float32 so due to the roundoff error there r slight dicrepancies but since it takes less space, its used in places where memory is a scarce resource."
  },
  {
    "objectID": "pytorch-experiments/basic_tensor_one.html#probably-ndim-gives-the-number-of-dimensions-and-shape-gives-us-the-dimensions-of-the-tensoror-matrix-in-the-form-of-nxmx....",
    "href": "pytorch-experiments/basic_tensor_one.html#probably-ndim-gives-the-number-of-dimensions-and-shape-gives-us-the-dimensions-of-the-tensoror-matrix-in-the-form-of-nxmx....",
    "title": "9  Tensor operation exercises",
    "section": "9.1 Probably, ndim gives the number of dimensions and shape gives us the dimensions of the tensor(or matrix) in the form of \\(n\\)x\\(m\\)x\\(...\\).",
    "text": "9.1 Probably, ndim gives the number of dimensions and shape gives us the dimensions of the tensor(or matrix) in the form of \\(n\\)x\\(m\\)x\\(...\\).\n\nTENSOR=torch.tensor([[[473,478,37],\n                      [273,472,72],\n                      [72819,47,281]]])\nTENSOR\n\ntensor([[[  473,   478,    37],\n         [  273,   472,    72],\n         [72819,    47,   281]]])\n\n\n\nTENSOR.ndim\n\n3\n\n\n\nTENSOR.shape\n\ntorch.Size([1, 3, 3])\n\n\n\nTENSOR2=torch.tensor([[[2,3,73],[27,73,21],\n                      [27,65,36],[10,738,98],\n                      [472,73,63],[73,482,63]],[[2,3,73],[27,73,21],\n                      [27,65,36],[10,738,98],\n                      [472,73,63],[73,482,63]]])\nTENSOR2\n\ntensor([[[  2,   3,  73],\n         [ 27,  73,  21],\n         [ 27,  65,  36],\n         [ 10, 738,  98],\n         [472,  73,  63],\n         [ 73, 482,  63]],\n\n        [[  2,   3,  73],\n         [ 27,  73,  21],\n         [ 27,  65,  36],\n         [ 10, 738,  98],\n         [472,  73,  63],\n         [ 73, 482,  63]]])\n\n\n\nTENSOR2.ndim\n\n3\n\n\n\nTENSOR2.shape\n\ntorch.Size([2, 6, 3])\n\n\nBy Convention, we use all caps for tensors and matrices and lower case for vectors and scalars. There are no terms as scalars, vectors, matrices in the syntax. All of them are tensors. The shape and dimensions of the tensor define the term used.\nTo Summarize\n\n\n\n\n\n\n\n\n\nName\nWhat is it?\nNumber of dimensions\nLower or upper (usually/example)\n\n\n\n\nscalar\na single number\n0\nLower (a)\n\n\nvector\na number with direction (e.g. wind speed with direction) but can also have many other numbers\n1\nLower (y)\n\n\nmatrix\na 2-dimensional array of numbers\n2\nUpper (Q)\n\n\ntensor\nan n-dimensional array of numbers\ncan be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector\nUpper (X)\n\n\n\n\n# Random tensors\nrand_ten= torch.rand(size=(5,3))\nrand_ten,rand_ten.dtype\n\n(tensor([[0.9307, 0.2047, 0.9102],\n         [0.7537, 0.7382, 0.4093],\n         [0.8215, 0.6938, 0.3436],\n         [0.6726, 0.6349, 0.0772],\n         [0.1314, 0.2939, 0.3545]]),\n torch.float32)\n\n\n\nrand_ten.dtype\n\ntorch.float32\n\n\nFor Example, if we want to create a tensor for the common image shape of (224, 224, 3) ie, (Height, Width, Color Channels).\n\nrand_image=torch.rand(size=(3,224,224))\nrand_image\n\ntensor([[[0.7235, 0.3475, 0.1128,  ..., 0.7870, 0.7319, 0.6244],\n         [0.9130, 0.7502, 0.0262,  ..., 0.5601, 0.3764, 0.1332],\n         [0.0326, 0.0666, 0.0381,  ..., 0.5440, 0.0450, 0.7834],\n         ...,\n         [0.3312, 0.3437, 0.1137,  ..., 0.1090, 0.7219, 0.3526],\n         [0.7155, 0.0402, 0.0209,  ..., 0.2060, 0.9748, 0.8547],\n         [0.4337, 0.9035, 0.8697,  ..., 0.0928, 0.6882, 0.6823]],\n\n        [[0.3749, 0.1274, 0.0679,  ..., 0.9208, 0.5695, 0.1757],\n         [0.6695, 0.3983, 0.9633,  ..., 0.1439, 0.8342, 0.6253],\n         [0.7849, 0.3406, 0.0078,  ..., 0.8696, 0.3369, 0.3489],\n         ...,\n         [0.9902, 0.3673, 0.7743,  ..., 0.1442, 0.0641, 0.2402],\n         [0.9773, 0.6198, 0.4050,  ..., 0.0056, 0.1235, 0.4614],\n         [0.1538, 0.6105, 0.3765,  ..., 0.9220, 0.4059, 0.7821]],\n\n        [[0.9005, 0.4082, 0.3391,  ..., 0.3896, 0.9649, 0.3884],\n         [0.1506, 0.6456, 0.9188,  ..., 0.2503, 0.1541, 0.5787],\n         [0.9122, 0.1805, 0.5534,  ..., 0.4106, 0.3451, 0.9317],\n         ...,\n         [0.5601, 0.9080, 0.5195,  ..., 0.4821, 0.3404, 0.2160],\n         [0.1838, 0.3120, 0.9754,  ..., 0.7687, 0.1310, 0.8943],\n         [0.3991, 0.7130, 0.2392,  ..., 0.6545, 0.3912, 0.6865]]])\n\n\n\nrand_image.ndim\n\n3\n\n\n\nrand_image.shape\n\ntorch.Size([3, 224, 224])\n\n\nTo create a tensor with just aeros (used to mask values which we dont need the model to learn)\n\nzeros=torch.zeros(size=(5,4))\n\n\nzeros\n\ntensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n\n\nzeros.dtype\n\ntorch.float32\n\n\n\nones=torch.ones(size=(5,4))\n\n\nones\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\n\nones.dtype\n\ntorch.float32\n\n\n\nzero_ten=torch.arange(start=0,end=10,step=1)\nzero_ten\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nzero_ten_three=torch.arange(start=0,end=10,step=3)\nzero_ten_three\n\ntensor([0, 3, 6, 9])\n\n\nTo create a tensor with same shape as another :\n\nten_ones=torch.ones_like(input=zero_ten_three)\nten_ones\n\ntensor([1, 1, 1, 1])\n\n\n\ntensor_float_32=torch.tensor([3.0,6.0,1.0],\n                            dtype=None)\ntensor_float_32\n\ntensor([3., 6., 1.])\n\n\n\ntensor_float_32.dtype\n\ntorch.float32\n\n\n\ntensor_float_32.ndim\n\n1\n\n\n\ntensor_float_32.shape\n\ntorch.Size([3])\n\n\n\ntensor_float_32.device\n\ndevice(type='cpu')\n\n\n\ntensor_float_16=torch.tensor([3.0,6.0,10.2],\n                            dtype=torch.float16)\ntensor_float_16\n\ntensor([ 3.0000,  6.0000, 10.2031], dtype=torch.float16)"
  },
  {
    "objectID": "pytorch-experiments/basic_tensor_one.html#the-data-explorers-motto-visualize-visualize-visualize",
    "href": "pytorch-experiments/basic_tensor_one.html#the-data-explorers-motto-visualize-visualize-visualize",
    "title": "9  Tensor operation exercises",
    "section": "10.1 The data explorer’s motto : VISUALIZE, VISUALIZE, VISUALIZE",
    "text": "10.1 The data explorer’s motto : VISUALIZE, VISUALIZE, VISUALIZE\n\n10.1.1 PyTorch and NumPy\nPyTorch and NumPy have fucntionality to interact pretty nicely.\nTo convert NumPy arrays to Pytorch tensors and vice-versa: * torch.from_numpy(ndarray) - NumPy array -> PyTorch tensor. * torch.Tensor.numpy() - PyTorch tensor -> NumPy array.\nFrom an array to a tensor :\n\nimport numpy as np\narr=np.arange(1,10)\ntens=torch.from_numpy(arr)\narr, tens\n\n(array([1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]))\n\n\nThe default datatype for NumPy is float64 so when we convert a NumPy array to a PyTorch tensor it retains the float64 datatype. But many PyTorch calculations default to float32.\nSo to convert a NumPy array(float64) to a PyTorch tensor(float64) to a Pytorch tensor(float32) we do tensor = torch.from_numpy(array).type(torch.float32)\n\narr.dtype, tens.dtype\n\n(dtype('int64'), torch.int64)\n\n\n\ntens=torch.from_numpy(arr).type(torch.float32)\ntens.dtype\n\ntorch.float32\n\n\nFrom a tensor to an array :\n\nnumpy_tens=tens.numpy()\ntens,numpy_tens\n\n(tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n array([1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32))\n\n\n\ntens.dtype, numpy_tens.dtype\n\n(torch.float32, dtype('float32'))\n\n\nFor both tensors and arrays converted from each other when we change any one theres no change in the other\n\nnumpy_tens+1,tens\n\n(array([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], dtype=float32),\n tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]))\n\n\n\nnumpy_tens+1==tens\n\nFalse\n\n\n\narr+1,tens\n\n(array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]))\n\n\n\narr+1==tens\n\nFalse\n\n\n\n\n10.1.2 Reproducibility\nTo get the same random tensors to make the code reproducable, we use seeds and torch.manual_seed(seed).\n\ntorch.manual_seed(seed=42)\nrand1=torch.rand(2,3,4)\nrand3=torch.rand(3,4,5)\nrand1,rand3\n\n(tensor([[[0.8823, 0.9150, 0.3829, 0.9593],\n          [0.3904, 0.6009, 0.2566, 0.7936],\n          [0.9408, 0.1332, 0.9346, 0.5936]],\n \n         [[0.8694, 0.5677, 0.7411, 0.4294],\n          [0.8854, 0.5739, 0.2666, 0.6274],\n          [0.2696, 0.4414, 0.2969, 0.8317]]]),\n tensor([[[0.1053, 0.2695, 0.3588, 0.1994, 0.5472],\n          [0.0062, 0.9516, 0.0753, 0.8860, 0.5832],\n          [0.3376, 0.8090, 0.5779, 0.9040, 0.5547],\n          [0.3423, 0.6343, 0.3644, 0.7104, 0.9464]],\n \n         [[0.7890, 0.2814, 0.7886, 0.5895, 0.7539],\n          [0.1952, 0.0050, 0.3068, 0.1165, 0.9103],\n          [0.6440, 0.7071, 0.6581, 0.4913, 0.8913],\n          [0.1447, 0.5315, 0.1587, 0.6542, 0.3278]],\n \n         [[0.6532, 0.3958, 0.9147, 0.2036, 0.2018],\n          [0.2018, 0.9497, 0.6666, 0.9811, 0.0874],\n          [0.0041, 0.1088, 0.1637, 0.7025, 0.6790],\n          [0.9155, 0.2418, 0.1591, 0.7653, 0.2979]]]))\n\n\n\ntorch.manual_seed(seed=42)\nrand2=torch.rand(2,3,4)\nrand4=torch.rand(3,4,5)\nrand2,rand4\n\n(tensor([[[0.8823, 0.9150, 0.3829, 0.9593],\n          [0.3904, 0.6009, 0.2566, 0.7936],\n          [0.9408, 0.1332, 0.9346, 0.5936]],\n \n         [[0.8694, 0.5677, 0.7411, 0.4294],\n          [0.8854, 0.5739, 0.2666, 0.6274],\n          [0.2696, 0.4414, 0.2969, 0.8317]]]),\n tensor([[[0.1053, 0.2695, 0.3588, 0.1994, 0.5472],\n          [0.0062, 0.9516, 0.0753, 0.8860, 0.5832],\n          [0.3376, 0.8090, 0.5779, 0.9040, 0.5547],\n          [0.3423, 0.6343, 0.3644, 0.7104, 0.9464]],\n \n         [[0.7890, 0.2814, 0.7886, 0.5895, 0.7539],\n          [0.1952, 0.0050, 0.3068, 0.1165, 0.9103],\n          [0.6440, 0.7071, 0.6581, 0.4913, 0.8913],\n          [0.1447, 0.5315, 0.1587, 0.6542, 0.3278]],\n \n         [[0.6532, 0.3958, 0.9147, 0.2036, 0.2018],\n          [0.2018, 0.9497, 0.6666, 0.9811, 0.0874],\n          [0.0041, 0.1088, 0.1637, 0.7025, 0.6790],\n          [0.9155, 0.2418, 0.1591, 0.7653, 0.2979]]]))\n\n\nWe need to reset the seed every time to get the same random tensors each time. Each seed gives the same rand tensors on same number of calls.\n\ntorch.manual_seed(seed=1)\nrando=torch.rand(2,3,4)\nrando\n\ntensor([[[0.7576, 0.2793, 0.4031, 0.7347],\n         [0.0293, 0.7999, 0.3971, 0.7544],\n         [0.5695, 0.4388, 0.6387, 0.5247]],\n\n        [[0.6826, 0.3051, 0.4635, 0.4550],\n         [0.5725, 0.4980, 0.9371, 0.6556],\n         [0.3138, 0.1980, 0.4162, 0.2843]]])\n\n\nChanging the seed changes the random tensors or the flavour of the randomness. All randomness in a computer is pseudo-randomness since a computer itself is a deterministic machine but some theories disagree."
  },
  {
    "objectID": "pytorch-experiments/basic_tensor_one.html#gpu",
    "href": "pytorch-experiments/basic_tensor_one.html#gpu",
    "title": "9  Tensor operation exercises",
    "section": "10.2 GPU",
    "text": "10.2 GPU\n\n!nvidia-smi\n\n/bin/bash: line 1: nvidia-smi: command not found\n\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice=\"cuda\" if torch.cuda.is_available else \"cpu\"\ndevice\n\n'cuda'\n\n\n\ntens=torch.rand(2,3)\ntens_gpu=tens.to(device)\ntens_gpu"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "CS7015. 2018. https://nptel.ac.in/courses/106106184.\n\n\nMcCulloch, and Pitts. 1943. https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf."
  }
]